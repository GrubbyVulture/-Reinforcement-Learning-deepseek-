{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d7627-3662-46de-851a-aa8d7322bf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 截断问题较为严重\n",
    "import re\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import trl\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "按照如下格式生成：\n",
    "<think>\n",
    "...\n",
    "</think>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def process_data(data):\n",
    "    \"\"\"处理数据集,确保字段名称正确\"\"\"\n",
    "    def format_sample(x):\n",
    "        return {\n",
    "            'prompt': [  # GRPO内部使用'prompt'字段\n",
    "                {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "                {'role': 'user', 'content': x['question_zh-cn']}\n",
    "            ],\n",
    "            'answer': x['answer_only']  # 这个字段会被传递给reward函数的kwargs\n",
    "        }\n",
    "    \n",
    "    data = data.map(format_sample)\n",
    "    # 移除原始字段,只保留需要的字段\n",
    "    data = data.remove_columns([col for col in data.column_names if col not in ['prompt', 'answer']])\n",
    "    return data\n",
    "    \n",
    "def extract_answer(text):\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def mark_num(text):\n",
    "    \"\"\"计算格式标记的奖励\"\"\"\n",
    "    reward = 0\n",
    "    if text.count(\"<think>\\n\") == 1:\n",
    "        reward += 0.125\n",
    "        \n",
    "    if text.count(\"</think>\\n\") == 1:\n",
    "        reward += 0.125\n",
    "        \n",
    "    if text.count(\"<answer>\\n\") == 1:\n",
    "        reward += 0.125\n",
    "        \n",
    "    if text.count(\"</answer>\\n\") == 1:\n",
    "        reward += 0.125\n",
    "    return reward\n",
    "\n",
    "def correctness_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    生成答案是否正确的奖励\n",
    "    \n",
    "    参数:\n",
    "        completions: list[list[dict]] - 模型生成的完成内容\n",
    "        **kwargs: 包含'prompts'和数据集中的额外字段,如'answer'\n",
    "    \"\"\"\n",
    "    # 提取参数\n",
    "    prompts = kwargs.get('prompts', [])\n",
    "    answer = kwargs.get('answer', [])\n",
    "    \n",
    "    # 调试信息\n",
    "    print(f\"\\nDEBUG - kwargs keys: {kwargs.keys()}\")\n",
    "    print(f\"DEBUG - answer: {answer}\")\n",
    "    print(f\"DEBUG - completions length: {len(completions)}\")\n",
    "    print(f\"DEBUG - prompts length: {len(prompts)}\")\n",
    "    \n",
    "    # 提取模型生成的文本\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_answer(r) for r in responses]\n",
    "    \n",
    "    # 调试信息\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    if prompts and len(prompts) > 0 and len(prompts[0]) > 0:\n",
    "        print(f\"问题:\\n{prompts[0][-1]['content']}\")\n",
    "    if answer and len(answer) > 0:\n",
    "        print(f\"\\n正确答案: {answer[0]}\")\n",
    "    print(f\"\\n模型输出:\\n{responses[0][:500]}...\")  # 只打印前500字符\n",
    "    print(f\"\\n提取后的答案: {extracted_responses[0]}\")\n",
    "    \n",
    "    # 计算奖励\n",
    "    rewards = [2.0 if response == str(ans) else 0.0\n",
    "        for response, ans in zip(extracted_responses, answer)]\n",
    "    \n",
    "    print(f\"正确性奖励: {rewards[0]}\")\n",
    "    print(f\"所有正确性奖励: {rewards}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "def digit_reward(completions, **kwargs):\n",
    "    \"\"\"生成答案是否是数字的奖励\"\"\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_answer(r) for r in responses]\n",
    "    rewards = [0.5 if response.isdigit() and response != \"\" else 0.0 \n",
    "            for response in extracted_responses]\n",
    "    print(f\"数字奖励: {rewards}\")\n",
    "    return rewards\n",
    "\n",
    "def hard_format_reward(completions, **kwargs):\n",
    "    \"\"\"严格格式奖励\"\"\"\n",
    "    pattern = r\"^<think>\\n.*?\\n</think>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, response, re.S) for response in responses]\n",
    "    rewards = [0.5 if match else 0.0 for match in matches]\n",
    "    print(f\"严格格式奖励: {rewards}\")\n",
    "    return rewards\n",
    "\n",
    "def soft_format_reward(completions, **kwargs):\n",
    "    \"\"\"宽松格式奖励\"\"\"\n",
    "    pattern = r\"<think>.*?</think>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.search(pattern, response, re.S) for response in responses]\n",
    "    rewards = [0.5 if match else 0.0 for match in matches]\n",
    "    print(f\"宽松格式奖励: {rewards}\")\n",
    "    return rewards\n",
    "\n",
    "def mark_reward(completions, **kwargs):\n",
    "    \"\"\"标记奖励（改善格式奖励稀疏问题）\"\"\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    rewards = [mark_num(response) for response in responses]\n",
    "    print(f\"标记奖励: {rewards}\")\n",
    "    return rewards\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model_name = \"/root/autodl-tmp/base_models/Qwen3-0.6B\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,  # 使用bf16减少显存\n",
    "        device_map=\"auto\"  # 自动分配设备\n",
    "    )\n",
    "    \n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # 加载并处理数据集\n",
    "    ds = load_dataset(\"/root/autodl-tmp/llm_study/deepseek_learn/datasets/gsm8k_chinese\")\n",
    "    data = process_data(ds['train'])\n",
    "    \n",
    "    # 打印数据集结构以验证\n",
    "    print(f\"数据集列名: {data.column_names}\")\n",
    "    print(f\"第一个样本的键: {data[0].keys()}\")\n",
    "    print(f\"第一个样本的prompt长度: {len(data[0]['prompt'])}\")\n",
    "    print(f\"第一个样本的answer: {data[0]['answer']}\")\n",
    "    \n",
    "    output_dir = \"output_v1_another\"\n",
    "\n",
    "    training_args = GRPOConfig(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=5e-6,\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.99,\n",
    "        weight_decay=0.1,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type='cosine',\n",
    "        logging_steps=1,\n",
    "        bf16=True,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=8,  # 增加到8以补偿减少的generation_batch_size\n",
    "        generation_batch_size=16,  # 从16降到8减少显存\n",
    "        num_generations=16,  # 从16降到8减少显存\n",
    "        max_prompt_length=256,\n",
    "        max_completion_length=768,\n",
    "        num_train_epochs=1,\n",
    "        save_steps=100,\n",
    "        max_grad_norm=0.1,\n",
    "        log_on_each_node=False,\n",
    "        use_vllm=False,\n",
    "        report_to=\"tensorboard\",\n",
    "        gradient_checkpointing=False,  # 启用梯度检查点减少显存\n",
    "    )\n",
    "    \n",
    "    trainer = GRPOTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        reward_funcs=[\n",
    "            mark_reward,\n",
    "            soft_format_reward,\n",
    "            hard_format_reward,\n",
    "            digit_reward,\n",
    "            correctness_reward\n",
    "        ],\n",
    "        args=training_args,\n",
    "        train_dataset=data,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n开始训练...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    print(f\"\\n训练完成,保存模型到 {output_dir}\")\n",
    "    trainer.save_model(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (zzt)",
   "language": "python",
   "name": "zzt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
